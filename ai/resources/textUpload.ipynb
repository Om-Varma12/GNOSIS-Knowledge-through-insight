{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7695577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import torch\n",
    "import clip\n",
    "from qdrant_client import QdrantClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from qdrant_client.models import PointStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f14e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys, qdrant_client\n",
    "# from importlib.metadata import version\n",
    "\n",
    "# print(\"Python exe:\", sys.executable)\n",
    "# print(\"Qdrant path:\", qdrant_client.__file__)\n",
    "# print(\"Qdrant version:\", version(\"qdrant-client\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01058913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9278234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "QDRANT_KEY = os.getenv(\"QDRANT_KEY\")\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "\n",
    "client = QdrantClient(\n",
    "    url = QDRANT_URL,\n",
    "    api_key = QDRANT_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d28eee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=36448 points_count=42706 segments_count=2 config=CollectionConfig(params=CollectionParams(vectors={'text': VectorParams(size=512, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), 'vision': VectorParams(size=512, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}, shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=10000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "# client.delete_collection(\"GNOSIS\")\n",
    "\n",
    "# from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# client.create_collection(\n",
    "#     collection_name=\"GNOSIS\",\n",
    "#     vectors_config={\n",
    "#         \"text\": VectorParams(size=512, distance=Distance.COSINE),\n",
    "#         \"vision\": VectorParams(size=512, distance=Distance.COSINE),\n",
    "#     }\n",
    "# )\n",
    "\n",
    "print(client.get_collection(\"GNOSIS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10e2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/STUDY/PROJECTS/GNOSIS/Resources/FIR file.csv\")\n",
    "# df.head()\n",
    "\n",
    "points = [] # store points to be uploaded in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee20e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ef5da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=200):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afb54ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"GNOSIS\"\n",
    "DOMAIN = \"medical\"\n",
    "MODALITY = \"article\"\n",
    "DATASET_NAME = \"Medical_Demo\"\n",
    "LANGUAGE = \"en\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "UPLOAD_BATCH = 1000\n",
    "\n",
    "points_buffer = []\n",
    "text_batch = []\n",
    "meta_batch = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a23aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 3508.83it/s]\n"
     ]
    }
   ],
   "source": [
    "for row_idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "\n",
    "    title = str(row[\"title\"]) if not pd.isna(row[\"title\"]) else \"\"\n",
    "    content = str(row[\"text\"]) if not pd.isna(row[\"text\"]) else \"\"\n",
    "\n",
    "    source_url = str(row[\"url\"]) if not pd.isna(row[\"url\"]) else \"\"\n",
    "    date = str(row[\"date\"]) if not pd.isna(row[\"date\"]) else \"\"\n",
    "    label = str(row[\"label\"]) if not pd.isna(row[\"label\"]) else \"\"\n",
    "\n",
    "    if not content.strip():\n",
    "        continue\n",
    "\n",
    "    doc_id = f\"medical_{row_idx}\"\n",
    "\n",
    "    # =========================\n",
    "    # Build document\n",
    "    # =========================\n",
    "    full_text = f\"\"\"\n",
    "    Title: {title}\n",
    "\n",
    "    Article:\n",
    "    {content}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # =========================\n",
    "    # Chunk\n",
    "    # =========================\n",
    "    chunks = chunk_text(full_text, max_words=200)\n",
    "\n",
    "    for chunk_id, chunk in enumerate(chunks):\n",
    "        text_batch.append(chunk)\n",
    "\n",
    "        meta_batch.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": chunk_id,\n",
    "\n",
    "            \"domain\": DOMAIN,\n",
    "            \"modality\": MODALITY,\n",
    "            \"dataset\": DATASET_NAME,\n",
    "\n",
    "            \"title\": title,\n",
    "            \"source_url\": source_url,\n",
    "            \"date\": date,\n",
    "\n",
    "            \"label\": label,\n",
    "            \"language\": LANGUAGE\n",
    "        })\n",
    "\n",
    "    # =========================\n",
    "    # Embed & Upload\n",
    "    # =========================\n",
    "    if len(text_batch) >= BATCH_SIZE:\n",
    "\n",
    "        tokens = clip.tokenize(text_batch, truncate=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vecs = model.encode_text(tokens).cpu().numpy()\n",
    "\n",
    "        for vec, meta, text_ in zip(vecs, meta_batch, text_batch):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector={\n",
    "                    \"text\": vec.tolist()\n",
    "                },\n",
    "                payload={\n",
    "                    **meta,\n",
    "                    \"chunk_text\": text_\n",
    "                }\n",
    "            )\n",
    "            points_buffer.append(point)\n",
    "\n",
    "        text_batch = []\n",
    "        meta_batch = []\n",
    "\n",
    "        if len(points_buffer) >= UPLOAD_BATCH:\n",
    "            client.upsert(collection_name=COLLECTION_NAME, points=points_buffer)\n",
    "            points_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950eab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if text_batch:\n",
    "    tokens = clip.tokenize(text_batch, truncate=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vecs = model.encode_text(tokens).cpu().numpy()\n",
    "\n",
    "    for vec, meta, text_ in zip(vecs, meta_batch, text_batch):\n",
    "        point = PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector={\n",
    "                \"text\": vec.tolist()\n",
    "            },\n",
    "            payload={\n",
    "                **meta,\n",
    "                \"chunk_text\": text_\n",
    "            }\n",
    "        )\n",
    "\n",
    "        points_buffer.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d3c53c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ingestion completed.\n"
     ]
    }
   ],
   "source": [
    "if points_buffer:\n",
    "    client.upsert(collection_name=COLLECTION_NAME, points=points_buffer)\n",
    "\n",
    "print(\"✅ Ingestion completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8892ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_payload(payload: dict, score: float):\n",
    "    out = {\n",
    "        \"label\": \"\",\n",
    "        \"title\": \"\",\n",
    "        \"text\": \"\",\n",
    "        \"date\": \"\",\n",
    "        \"url\": \"\",\n",
    "        \"image_url\": \"\",\n",
    "        \"video_url\": \"\",\n",
    "        \"score\": float(score),\n",
    "    }\n",
    "\n",
    "    if isinstance(payload, dict):\n",
    "        out[\"label\"] = payload.get(\"label\", \"\") or \"\"\n",
    "        out[\"title\"] = payload.get(\"title\", \"\") or \"\"\n",
    "        out[\"date\"] = payload.get(\"date\", \"\") or \"\"\n",
    "        out[\"text\"] = payload.get(\"chunk_text\") or \"\"\n",
    "        out[\"url\"] = payload.get(\"source_url\") or \"\"\n",
    "        out[\"image_url\"] = payload.get(\"image_url\", \"\") or \"\"\n",
    "        out[\"video_url\"] = payload.get(\"video_url\", \"\") or \"\"\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2914b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TEXT = \"Sonu Sood for fake charity and improper funds use\"\n",
    "\n",
    "tokens = clip.tokenize([QUERY_TEXT], truncate=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    vec = model.encode_text(tokens).cpu().numpy()[0]\n",
    "\n",
    "result = client.query_points(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query=vec.tolist(),\n",
    "    using=\"text\",     \n",
    "    limit=5,\n",
    "    with_payload=True\n",
    ")\n",
    "\n",
    "hits = result.points\n",
    "\n",
    "summary = {\n",
    "    \"fake\": {\"count\": 0, \"items\": []},\n",
    "    \"real\": {\"count\": 0, \"items\": []},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13374ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hit in hits:\n",
    "    item = normalize_payload(hit.payload, hit.score)\n",
    "    label = (item[\"label\"] or \"\").lower().strip()\n",
    "\n",
    "    if label in summary:\n",
    "        summary[label][\"count\"] += 1\n",
    "        summary[label][\"items\"].append(item)\n",
    "\n",
    "if summary[\"fake\"][\"count\"] > summary[\"real\"][\"count\"]:\n",
    "    final_verdict = \"fake\"\n",
    "elif summary[\"real\"][\"count\"] > summary[\"fake\"][\"count\"]:\n",
    "    final_verdict = \"real\"\n",
    "else:\n",
    "    final_verdict = \"uncertain\"\n",
    "\n",
    "final_output = {\n",
    "    \"final_verdict\": final_verdict,\n",
    "    \"fake\": summary[\"fake\"],\n",
    "    \"real\": summary[\"real\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb62d3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"final_verdict\": \"fake\",\n",
      "  \"fake\": {\n",
      "    \"count\": 5,\n",
      "    \"items\": [\n",
      "      {\n",
      "        \"label\": \"fake \",\n",
      "        \"title\": \"Sonu Sood exposed for fake charity spending during pandemic\",\n",
      "        \"text\": \"Title: Sonu Sood exposed for fake charity spending during pandemic Article: Viral posts claim IT exposed Sonu Sood for fake charity and improper funds use, donating only a fraction of money collected.\",\n",
      "        \"date\": \"08-08-2021\",\n",
      "        \"url\": \"https://reddit.com/sood-fake-charity\",\n",
      "        \"image_url\": \"\",\n",
      "        \"video_url\": \"\",\n",
      "        \"score\": 0.8965769\n",
      "      },\n",
      "      {\n",
      "        \"label\": \"fake\",\n",
      "        \"title\": \"Did Sonu Sood misuse charity funds of ₹19 crore?\",\n",
      "        \"text\": \"Title: Did Sonu Sood misuse charity funds of ₹19 crore? Article: Blog post claiming evidence of Sonu Sood misusing ₹19 cr COVID-19 donations with minimal actual aid.\",\n",
      "        \"date\": \"30-07-2021\",\n",
      "        \"url\": \"https://healthrumors.net/sood-misuse\",\n",
      "        \"image_url\": \"\",\n",
      "        \"video_url\": \"\",\n",
      "        \"score\": 0.8404089\n",
      "      },\n",
      "      {\n",
      "        \"label\": \"fake\",\n",
      "        \"title\": \"Celebrity fraud: Sonu Sood kept COVID relief funds\",\n",
      "        \"text\": \"Title: Celebrity fraud: Sonu Sood kept COVID relief funds Article: A message chain asserts Sood personally kept COVID relief money, only giving small amounts to needy people.\",\n",
      "        \"date\": \"22-06-2021\",\n",
      "        \"url\": \"whatsapp://share/sood-covid-fraud\",\n",
      "        \"image_url\": \"\",\n",
      "        \"video_url\": \"\",\n",
      "        \"score\": 0.82128066\n",
      "      },\n",
      "      {\n",
      "        \"label\": \"fake\",\n",
      "        \"title\": \"Sonu Sood “fake helper” rumors during COVID\",\n",
      "        \"text\": \"Title: Sonu Sood “fake helper” rumors during COVID Article: Meme post claiming Sood’s famous help was exaggerated and most money wasn’t actually spent for aid.\",\n",
      "        \"date\": \"18-08-2021\",\n",
      "        \"url\": \"https://insta.com/reel/sood-fake\",\n",
      "        \"image_url\": \"\",\n",
      "        \"video_url\": \"\",\n",
      "        \"score\": 0.82020223\n",
      "      },\n",
      "      {\n",
      "        \"label\": \"fake\",\n",
      "        \"title\": \"Sonu Sood took ₹19 cr donations but gave only ₹2-3 cr\",\n",
      "        \"text\": \"Title: Sonu Sood took ₹19 cr donations but gave only ₹2-3 cr Article: A widely shared claim alleges Sonu Sood collected ₹19 cr in donations during COVID-19 relief but only donated ₹2-3 cr, implying misuse of funds.\",\n",
      "        \"date\": \"15-07-2021\",\n",
      "        \"url\": \"https://example.com/sood-donations-19cr\",\n",
      "        \"image_url\": \"\",\n",
      "        \"video_url\": \"\",\n",
      "        \"score\": 0.8019215\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"real\": {\n",
      "    \"count\": 0,\n",
      "    \"items\": []\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(final_output, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2ef51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iitGnosisDataset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
